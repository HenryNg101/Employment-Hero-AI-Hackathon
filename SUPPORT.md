# Employment Hero Copilot â€” Supporting Documentation

### ğŸ”§ Setup & Project Overview

This prototype is a **multi-agent Copilot** designed to help employees and HR teams by answering policy questions, assisting with onboarding/domain knowledge, and digesting messy documentation.

#### Project Structure

* **Frontend** â†’ Next.js + Typescript chat app (supports file upload)
* **Backend** â†’ FastAPI Python server (handles agents, embeddings, and retrieval)
* **Vector Database** â†’ ChromaDB for document storage & semantic search
* **LLM Engine** â†’ Ollama (tested with `llama3.1:8b` model)
* **Embedding Model** â†’ Sentence-transformers from HuggingFace for semantic chunking
* **Auth** â†’ AWS Amplify for authentication

#### Local Setup

1. Clone the repository
2. Start backend:

   ```bash
   cd backend 

   # Pre-load pre-processed mock chunks information from json files
   python ingest_chunks.py docs ./docs_chunks.json
   python ingest_chunks.py domain ./domain_chunks.json
   python ingest_chunks.py policy ./policy_chunks.json

   # Start the server
   uvicorn main:app --reload  
   ```
3. Start frontend:

   ```bash
   cd frontend  
   npm install  
   npm run dev  
   ```
4. Ensure Ollama is running locally with the chosen model pulled (e.g., `ollama pull llama3.1:8b`)

---

### ğŸ§  Technical Flow

1. **User Uploads a File** â†’ Document is parsed into semantic chunks (sentence-transformer).
2. **Chunks Stored in ChromaDB** â†’ Each chunk gets embedded for fast retrieval.
3. **User Asks a Question** â†’ Backend queries ChromaDB for relevant chunks.
4. **Ollama LLM** â†’ Uses retrieved chunks as context to generate human-like answers.
5. **Response** â†’ Sent back to the frontend chat app.

---

### âš¡ Challenges Faced

* **Performance Bottleneck** â†’ Semantic chunking and embedding large files is slow. Using `llama3.1:8b` on Ollama adds additional latency.
* **File Upload Bugs** â†’ Fake chunks worked fine, but real uploaded documents werenâ€™t always processed correctly (retrieval gaps).
* **UI/UX** â†’ Chat UI is functional but minimal â€” more like a skeleton prototype than polished product.
* **Limited Time** â†’ Hackathon constraints meant some features (multi-agent switching, advanced insights) are only partially implemented or mocked.

---

### ğŸ”® Future Work

* **Better Chunking** â†’ Explore adaptive chunk sizes and hybrid approaches (semantic + structural) to improve retrieval accuracy.
* **Model Optimization** â†’ Try smaller, faster LLMs (e.g., `llama3.1:3b`, `mistral`, or OpenAI API if budget allows) to improve speed and responsiveness.
* **Multi-Agent Coordination** â†’ Extend current system with a few agents chat into a true multi-agent system where different specialists collaborate (policy, docs, domain knowledge).
* **Polished UX** â†’ Add styling, persona switching (choose â€œPolicy Copilotâ€ or â€œDocs Copilotâ€), and smoother demo experience.
* **Cost Efficiency** â†’ Experiment with hybrid local + cloud inference for balance between speed and cost.

---

### ğŸ¯ Key Takeaway

Even in its early state, the Copilot **proves the feasibility of multi-agent AI inside Employment Heroâ€™s ecosystem**. With further refinement, it can scale from a working prototype into a robust AI assistant that reduces HR overhead, accelerates onboarding, and supports employees 24/7.